# -*- coding: utf-8 -*-
"""eeg_main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CnRFvkKZx46Y1v7A4xBkML_HCgeUj9hF
"""

from google.colab import drive
import sys
import os
import numpy as np
drive.mount('/content/drive')

PROJECT_PATH = '/content/drive/MyDrive/Advance_python_project'
if PROJECT_PATH not in sys.path:
    sys.path.append(PROJECT_PATH)
!pip install torchcontrib pytorch-lightning==1.9.5 librosa sh

"""# New code with paper's specifications"""

# --- IMPORTS ---
import collections
import time
import random
import os
import sys
import glob
import torch
import torch.nn.functional as F
import pytorch_lightning as pl
import numpy as np
from absl import logging
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping

# Import modules from Drive
import dataset
import models
from torchcontrib.optim import SWA
from two_sample_distance import pdist

# --- CONFIGURATION ---
class LocalConfig:
    data_dir = PROJECT_PATH
    audio_dir = PROJECT_PATH
    patient_id = 'p3'

    batch_size = 512
    epochs = 50
    learning_rate = 0.0001
    lr_milestones = [45]

    teacher_forcing_ratio = 0.1
    hidden_size = 333
    n_layers = 3
    n_layers_decoder = 1
    dropout = 0.5
    n_pos = 32
    convolve_eeg_1d = True
    convolve_eeg_2d = False
    convolve_eeg_3d = False
    use_bahdanau_attention = True
    pre_and_postnet = True
    pre_and_postnet_dim = 256

    hop_in_ms = 25.0
    optim = 'Adam'
    laplace_smoothing = 1e-2
    gpus = 1
    use_MFCCs = True
    SWA = True
    swa_start = 100
    clean_logs_dir = False
    debug = False  # fast_dev_run is OFF

CFG = LocalConfig()

def run_training():
    # A100 Optimization
    torch.set_float32_matmul_precision('high')

    # --- FLAG INITIALIZATION ---
    try:
        dataset.FLAGS.mark_as_parsed()
        models.FLAGS.mark_as_parsed()
    except:
        dataset.FLAGS(sys.argv[:1])
        models.FLAGS(sys.argv[:1])

    dataset.FLAGS.data_dir = CFG.data_dir
    dataset.FLAGS.audio_dir = CFG.audio_dir
    dataset.FLAGS.patient_id = CFG.patient_id
    dataset.FLAGS.use_MFCCs = CFG.use_MFCCs

    models.FLAGS.hidden_size = CFG.hidden_size
    models.FLAGS.n_layers = CFG.n_layers
    models.FLAGS.dropout = CFG.dropout

    '''
    models.FLAGS.pre_and_postnet_dim = CFG.pre_and_postnet_dim
    models.FLAGS.convolve_eeg_1d = CFG.convolve_eeg_1d
    models.FLAGS.convolve_eeg_2d = CFG.convolve_eeg_2d
    models.FLAGS.convolve_eeg_3d = CFG.convolve_eeg_3d
    models.FLAGS.use_bahdanau_attention = CFG.use_bahdanau_attention
    models.FLAGS.pre_and_postnet = CFG.pre_and_postnet
    models.FLAGS.n_layers_decoder = CFG.n_layers_decoder
    models.FLAGS.n_pos = CFG.n_pos
    models.FLAGS.teacher_forcing_ratio = CFG.teacher_forcing_ratio
    models.FLAGS.optim = CFG.optim
    models.FLAGS.laplace_smoothing = CFG.laplace_smoothing
    models.FLAGS.hop_in_ms = CFG.hop_in
    models.FLAGS.debug = CFG.debug
    models.FLAGS.SWA = CFG.SWA
    models.FLAGS.swa_start = CFG.swa_start
    models.FLAGS.clean_logs_dir = CFG.clean_logs_dir
    models.FLAGS.gpus = CFG.gpus
    models.FLAGS.batch_size = CFG.batch_size
    models.FLAGS.epochs = CFG.epochs
    models.FLAGS.learning_rate = CFG.learning_rate
    models.FLAGS.lr_milestones = CFG.lr_milestones
    '''


    print(f"‚úÖ GPU Detected: {torch.cuda.get_device_name(0)}")

    # Load Data
    print(f"--- Loading Data for {CFG.patient_id} ---")
    train_ds = dataset.get_data(split='train', hop=CFG.hop_in_ms)
    test_ds = dataset.get_data(split='test')

    class Model(pl.LightningModule):
        def __init__(self):
            super().__init__()
            self.input_shape = train_ds[0][0].shape
            self.mel_transformer = train_ds.tacotron_mel_transformer

            dummy_audio = train_ds[0][1].unsqueeze(0)
            mel_spec = self.mel_transformer.mel_spectrogram(dummy_audio)
            self.output_shape = mel_spec.squeeze(0).T.shape
            self.criterion = torch.nn.MSELoss(reduction='none')

            self.seq2seq = models.RNNSeq2Seq(self.input_shape, self.output_shape)

        def loss(self, logits, y):
            return self.criterion(logits, y)

        def accuracy(self, logits, y):
            # Pearson correlation
            pearson_r = torch.nn.functional.cosine_similarity(
                y - torch.mean(y, dim=1).unsqueeze(1),
                logits - torch.mean(logits, dim=1).unsqueeze(1),
                dim=1
            )
            return pearson_r

        def training_step(self, batch, batch_idx):
            x, y = batch
            y = self.mel_transformer.mel_spectrogram(y).transpose(1, 2)

            teacher_forcing = (torch.rand(x.size(0), device=self.device) < CFG.teacher_forcing_ratio)

            logits, _, _ = self.seq2seq(x, y=y, teacher_forcing=teacher_forcing)
            loss = self.loss(logits, y).mean()
            self.log("loss/train", loss, prog_bar=True, on_epoch=True)
            return loss

        def validation_step(self, batch, batch_idx):
            x, y = batch
            y = self.mel_transformer.mel_spectrogram(y).transpose(1, 2)
            logits, _, _ = self.seq2seq(x)
            loss = self.loss(logits, y).mean()
            acc = self.accuracy(logits, y).mean()
            return {'val_loss': loss, 'pearson_r': acc, 'preds': logits, 'targets': y}

        def validation_epoch_end(self, outputs):
            avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
            avg_acc = torch.stack([x['pearson_r'] for x in outputs]).mean()

            epoch_num = self.current_epoch
            path = "/content/drive/MyDrive/Advance_python_project/mel_preds_run2"
            os.makedirs(path, exist_ok=True)

            # Using corrected .permute for batch saving
            torch.save(outputs[0]['preds'].permute(0, 2, 1), f"{path}/{CFG.patient_id}_preds_epoch_{epoch_num}.pt")
            torch.save(outputs[0]['targets'].permute(0, 2, 1), f"{path}/{CFG.patient_id}_targets_epoch_{epoch_num}.pt")

            self.log("val_loss", avg_loss, prog_bar=True)
            self.log("pearson_r/val", avg_acc, prog_bar=True)
            print(f"Epoch {epoch_num}: Val Loss={avg_loss:.4f}, Pearson R={avg_acc:.4f}")

        def train_dataloader(self):
            return torch.utils.data.DataLoader(train_ds, shuffle=True, drop_last=True,
                                             num_workers=4, pin_memory=True, batch_size=CFG.batch_size)

        def val_dataloader(self):
            return torch.utils.data.DataLoader(test_ds, shuffle=False, drop_last=False,
                                             num_workers=4, pin_memory=True, batch_size=CFG.batch_size)

        def configure_optimizers(self):
            optimizer = torch.optim.AdamW(self.parameters(), lr=CFG.learning_rate)
            if CFG.SWA:
                iters = int(len(train_ds)/CFG.batch_size)
                optimizer = SWA(optimizer, swa_start=int(CFG.swa_start*iters), swa_freq=50, swa_lr=CFG.learning_rate/10)
                # Patches for PyTorch 2.0+ Resume
                missing_attrs = [
                    '_optimizer_step_pre_hooks',       # <--- THIS WAS MISSING
                    '_optimizer_step_post_hooks',      # <--- THIS WAS MISSING
                    '_optimizer_load_state_dict_pre_hooks',
                    '_optimizer_load_state_dict_post_hooks',
                    '_optimizer_state_dict_pre_hooks',
                    '_optimizer_state_dict_post_hooks',
                    'defaults'
                ]

                for attr in missing_attrs:
                  if not hasattr(optimizer, attr):
                        setattr(optimizer, attr, collections.OrderedDict() if attr != 'defaults' else optimizer.optimizer.defaults)

            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=CFG.lr_milestones, gamma=0.5)
            return [optimizer], [scheduler]

    # --- PREPARE TRAINER & RESUME ---
    model = Model()
    drive_ckpt_dir = f"/content/drive/MyDrive/Advance_python_project/checkpoints_run2/{CFG.patient_id}"
    ckpts = glob.glob(f"{drive_ckpt_dir}/*.ckpt")
    resume_ckpt = max(ckpts, key=os.path.getmtime) if ckpts else None

    if resume_ckpt:
        print(f"üîÑ Resuming from: {os.path.basename(resume_ckpt)}")

    # We only keep the checkpoint callback.
    checkpoint_callback = ModelCheckpoint(dirpath=drive_ckpt_dir, save_last=True, monitor="val_loss")

    # üî¥ FIX APPLIED: early_stop_callback removed from this list
    trainer = pl.Trainer(
        accelerator='gpu',
        devices=1,
        max_epochs=CFG.epochs,
        callbacks=[checkpoint_callback], # ONLY CHECKPOINTING
        precision='bf16'
    )

    print(f"üöÄ Resuming Training (Target: 50 Epochs, No Early Stop)...")
    trainer.fit(model, ckpt_path=resume_ckpt)

run_training()

"""seeing performance metrics"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Launch TensorBoard pointing to your Drive logs
# This will show both p1 (if saved) and the new p2 logs
# %tensorboard --logdir /content/drive/MyDrive/Advance_python_project/saved_logs

"""# convert to waveglow

## patient 3
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
import matplotlib.pyplot as plt
import numpy as np
import scipy.io.wavfile
import os
import glob
from scipy.stats import pearsonr

# --- CONFIGURATION ---
DRIVE_FOLDER = '/content/drive/MyDrive/Advance_python_project'
PREDS_DIR = os.path.join(DRIVE_FOLDER, 'mel_preds_run2')
WAV_DIR = DRIVE_FOLDER
PATIENT_ID = 'p3'

# üî¥ MANUAL EPOCH SELECTION (Set this to 41 based on your table)
TARGET_EPOCH = 49

print(f"--- üè• Model Diagnosis for {PATIENT_ID} (Epoch {TARGET_EPOCH}) ---")

# 1. CONSTRUCT PATHS FOR TARGET EPOCH
latest_pt = os.path.join(PREDS_DIR, f"{PATIENT_ID}_preds_epoch_{TARGET_EPOCH}.pt")
latest_target_pt = os.path.join(PREDS_DIR, f"{PATIENT_ID}_targets_epoch_{TARGET_EPOCH}.pt")

# Check if files exist
if not os.path.exists(latest_pt):
    raise FileNotFoundError(f"‚ùå Prediction file missing: {latest_pt}")
if not os.path.exists(latest_target_pt):
    raise FileNotFoundError(f"‚ùå Target file missing: {latest_target_pt}")

print(f"Analyzing Prediction: {os.path.basename(latest_pt)}")
print(f"Looking for Target:   {os.path.basename(latest_target_pt)}")

# 2. FIND AUDIO FILES (Optional)
# (These likely won't exist yet for Epoch 41 until you run the generation script)
wav_pred_path = os.path.join(WAV_DIR, f"{PATIENT_ID}_epoch{TARGET_EPOCH}_BRAIN_FIXED.wav")
wav_true_path = os.path.join(WAV_DIR, f"{PATIENT_ID}_epoch{TARGET_EPOCH}_TRUTH_FIXED.wav")

# 3. LOAD DATA (Force Float32)
P_spec = torch.load(latest_pt, map_location='cpu').float()
T_spec = torch.load(latest_target_pt, map_location='cpu').float()

# Flatten to (80, Time)
# Logic to handle different saved orientations (Permuted vs Transposed)
if P_spec.shape[0] == 80:
    # If shape is (80, Time)
    P_flat = P_spec.numpy()
    T_flat = T_spec.numpy()
elif P_spec.shape[-1] == 80:
    # If shape is (Batch, Time, 80) -> Flatten and Transpose
    P_flat = P_spec.reshape(-1, 80).T.numpy()
    T_flat = T_spec.reshape(-1, 80).T.numpy()
else:
    # Fallback for (Batch, 80, Time)
    P_flat = P_spec.permute(0, 2, 1).reshape(-1, 80).T.numpy()
    T_flat = T_spec.permute(0, 2, 1).reshape(-1, 80).T.numpy()

# Load Audio (if exists)
has_audio = False
if os.path.exists(wav_pred_path):
    try:
        sr, P_wav = scipy.io.wavfile.read(wav_pred_path)
        _, T_wav = scipy.io.wavfile.read(wav_true_path)
        has_audio = True
    except:
        print("‚ö†Ô∏è Audio files found but could not be read.")

# 4. PLOT EVERYTHING
fig, axes = plt.subplots(nrows=3 if has_audio else 2, ncols=2, figsize=(20, 15))

# --- ROW 1: MEL SPECTROGRAMS (The "Brain") ---
vmin = T_flat.min()
vmax = T_flat.max()

im1 = axes[0, 0].imshow(T_flat[:, :1000], aspect='auto', origin='lower', cmap='inferno', vmin=vmin, vmax=vmax)
axes[0, 0].set_title(f"Ground Truth Spectrogram (Target)", fontsize=14, color='green')
axes[0, 0].set_ylabel("Freq Bands")
plt.colorbar(im1, ax=axes[0, 0])

im2 = axes[0, 1].imshow(P_flat[:, :1000], aspect='auto', origin='lower', cmap='inferno', vmin=vmin, vmax=vmax)
axes[0, 1].set_title(f"Predicted Spectrogram (Model Output)", fontsize=14, color='blue')
plt.colorbar(im2, ax=axes[0, 1])

# --- ROW 2: HISTOGRAM (The "Range" Check) ---
axes[1, 0].hist(T_flat.flatten(), bins=50, color='green', alpha=0.7, label='Truth')
axes[1, 0].hist(P_flat.flatten(), bins=50, color='blue', alpha=0.7, label='Pred')
axes[1, 0].legend()
axes[1, 0].set_title("Value Distribution (Histogram)", fontsize=14)
axes[1, 0].set_xlabel("Mel Value (Needs to be between -11 and 0 roughly)")

# Pearson Score Text
correlations = []
for i in range(80):
    if np.std(P_flat[i]) > 1e-6:
        c, _ = pearsonr(P_flat[i], T_flat[i])
        correlations.append(c)
avg_corr = np.mean(correlations)

axes[1, 1].axis('off')
axes[1, 1].text(0.1, 0.5, f" DIAGNOSTICS REPORT:\n\n"
                          f" Epoch: {TARGET_EPOCH}\n"
                          f" Pearson Correlation: {avg_corr:.4f}\n"
                          f" Truth Range: {T_flat.min():.2f} to {T_flat.max():.2f}\n"
                          f" Pred Range:  {P_flat.min():.2f} to {P_flat.max():.2f}\n\n"
                          f" INTERPRETATION:\n"
                          f" If Pred Range is tiny (e.g. -0.1 to 0.1) -> Model collapsed (Silence)\n"
                          f" If Correlation < 0.2 -> Model hasn't learned yet\n"
                          f" If Pred matches Truth but Audio is bad -> WaveGlow Issue",
                fontsize=16, family='monospace')

# --- ROW 3: WAVEFORMS (The "Sound") ---
if has_audio:
    limit = 17050 * 5 # First 5 seconds
    axes[2, 0].plot(T_wav[:limit], color='green', alpha=0.8)
    axes[2, 0].set_title("Ground Truth Audio Waveform", fontsize=14)
    axes[2, 0].set_ylim(np.min(T_wav), np.max(T_wav))

    axes[2, 1].plot(P_wav[:limit], color='blue', alpha=0.8)
    axes[2, 1].set_title("Predicted Audio Waveform", fontsize=14)
    axes[2, 1].set_ylim(np.min(T_wav), np.max(T_wav))

plt.tight_layout()
plt.show()

import torch
import matplotlib.pyplot as plt
import os

# CONFIGURATION
patient_id = 'p3'
epoch_to_view = 41  # Change this to look at different epochs
save_path = "/content/drive/MyDrive/Advance_python_project/mel_preds"

# 1. Load the Tensors
pred_path = f"{save_path}/{patient_id}_preds_epoch_{epoch_to_view}.pt"
target_path = f"{save_path}/{patient_id}_targets_epoch_{epoch_to_view}.pt"

if os.path.exists(pred_path) and os.path.exists(target_path):
    # Load (and detach from GPU if necessary)
    preds = torch.load(pred_path, map_location=torch.device('cpu'))
    targets = torch.load(target_path, map_location=torch.device('cpu'))

    # 2. Select a random sample from the batch to plot
    sample_idx = 0
f
    # --- FIX APPLIED HERE: Added .float() ---
    pred_sample = preds[sample_idx].detach().float().numpy()
    target_sample = targets[sample_idx].detach().float().numpy()
    # ----------------------------------------

    # 3. Plot
    fig, axes = plt.subplots(2, 1, figsize=(10, 8))

    # Target (Truth)
    im1 = axes[0].imshow(target_sample, aspect='auto', origin='lower', interpolation='none')
    axes[0].set_title(f"Ground Truth (Epoch {epoch_to_view})")
    axes[0].set_ylabel("Mel Channels")
    fig.colorbar(im1, ax=axes[0])

    # Prediction
    im2 = axes[1].imshow(pred_sample, aspect='auto', origin='lower', interpolation='none')
    axes[1].set_title(f"Prediction (Epoch {epoch_to_view})")
    axes[1].set_ylabel("Mel Channels")
    axes[1].set_xlabel("Time Frames")
    fig.colorbar(im2, ax=axes[1])

    plt.tight_layout()
    plt.show()

else:
    print(f"‚ùå Files not found for Epoch {epoch_to_view}.")
    print(f"Checked: {pred_path}")

"""convert patient 41 to audio"""

import torch
import numpy as np
import soundfile as sf
import sys
import os
import dataset
import models
import pytorch_lightning as pl
from absl import flags
from tqdm import tqdm

# --- CONFIGURATION ---
PATIENT_ID = 'p3'
PROJECT_PATH = "/content/drive/MyDrive/Advance_python_project"
CHECKPOINT_PATH = "/content/drive/MyDrive/Advance_python_project/checkpoints_run2/p3/last.ckpt"
OUTPUT_DIR = "/content/drive/MyDrive/Advance_python_project/p3_generated_audio_epoch49_RAW"

# --- 1. SET FLAGS TO MATCH REALITY (CRITICAL FIX) ---
# Unlock flags
try:
    flags.FLAGS(sys.argv[:1])
except flags.Error:
    pass

# Dataset Flags
dataset.FLAGS.data_dir = PROJECT_PATH
dataset.FLAGS.audio_dir = PROJECT_PATH
dataset.FLAGS.patient_id = PATIENT_ID

# üî¥ MODEL FLAGS üî¥
# We set these to match how the model was ACTUALLY trained (Default: False)
models.FLAGS.convolve_eeg_1d = False
models.FLAGS.hidden_size = 333
models.FLAGS.n_layers = 3
models.FLAGS.dropout = 0.5
models.FLAGS.pre_and_postnet_dim = 256 # Added just in case

# --- 2. SETUP MODELS ---
print("Loading Models...")
try:
    waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32', trust_repo=True)
    waveglow = waveglow.remove_weightnorm(waveglow).to('cuda').eval()
except Exception as e:
    sys.exit(f"‚ùå Error loading WaveGlow: {e}")

class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()
        # Load one sample to get shapes
        temp_ds = dataset.get_data(split='train', hop=25.0)
        self.input_shape = temp_ds[0][0].shape
        self.mel_transformer = temp_ds.tacotron_mel_transformer

        # Create dummy audio to calculate output shape
        dummy_audio = temp_ds[0][1].unsqueeze(0)
        mel_spec = self.mel_transformer.mel_spectrogram(dummy_audio)
        self.output_shape = mel_spec.squeeze(0).T.shape

        # Init Seq2Seq
        self.seq2seq = models.RNNSeq2Seq(self.input_shape, self.output_shape)

    def forward(self, x):
        outputs, _, _ = self.seq2seq(x)
        return outputs

# Load your trained model
print(f"Loading checkpoint from: {CHECKPOINT_PATH}")
# We use strict=True now because we are confident we matched the flags!
model = Model.load_from_checkpoint(CHECKPOINT_PATH, strict=True)
model.to('cuda').eval()

# --- 3. PREPARE TEST SET ---
test_ds = dataset.get_data(split='test')
total_samples = len(test_ds)
print(f"‚úÖ Loaded. Processing ALL {total_samples} samples from Test Set.")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Lists for concatenation
full_pred_audio = []
full_true_audio = []
silence = np.zeros(int(22050 * 0.5)) # 0.5s silence

# --- 4. GENERATION LOOP ---
for i in tqdm(range(total_samples), desc="Generating Audio"):

    eeg, target_audio = test_ds[i]
    eeg = eeg.unsqueeze(0).to('cuda')

    with torch.no_grad():
        pred_mel = model(eeg)
        pred_mel = pred_mel.transpose(1, 2)
        audio = waveglow.infer(pred_mel)

    audio_pred = audio[0].data.cpu().numpy()
    audio_true = target_audio.cpu().numpy()

    # Save individual file
    sf.write(f"{OUTPUT_DIR}/sample_{i}_PRED.wav", audio_pred, 22050)

    # Append for full file
    full_pred_audio.append(audio_pred)
    full_pred_audio.append(silence)
    full_true_audio.append(audio_true)
    full_true_audio.append(silence)

# --- 5. SAVE FULL FILES ---
print("Concatenating...")
final_pred = np.concatenate(full_pred_audio)
final_true = np.concatenate(full_true_audio)

sf.write(f"{OUTPUT_DIR}/FULL_TEST_PREDICTION_RAW.wav", final_pred, 22050)
sf.write(f"{OUTPUT_DIR}/FULL_TEST_TRUTH.wav", final_true, 22050)

print(f"\nüéâ DONE. Output saved to: {OUTPUT_DIR}")

"""## generate audio 2"""

import torch
import numpy as np
import soundfile as sf
import sys
import os
import dataset
import models
import pytorch_lightning as pl
from tqdm import tqdm # Progress bar

# --- CONFIGURATION ---
PATIENT_ID = 'p3'
# Point exactly to your best checkpoint
CHECKPOINT_PATH = "/content/drive/MyDrive/Advance_python_project/checkpoints_run2/p3/last.ckpt"
OUTPUT_DIR = "/content/drive/MyDrive/Advance_python_project/p3_generated_audio_epoch49_ORIGINAL"

# --- 1. SETUP MODELS ---
print("Loading Models...")
try:
    waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp32')
    waveglow = waveglow.remove_weightnorm(waveglow).to('cuda').eval()
except Exception as e:
    sys.exit(f"‚ùå Error loading WaveGlow: {e}")

class LocalConfig:
    data_dir = PROJECT_PATH
    audio_dir = PROJECT_PATH
    patient_id = PATIENT_ID
    use_MFCCs = True
    hop_in_ms = 25.0

class Model(pl.LightningModule):
    def __init__(self):
        super().__init__()
        # Load one sample to get shapes
        temp_ds = dataset.get_data(split='train', hop=25.0)
        self.input_shape = temp_ds[0][0].shape
        self.mel_transformer = temp_ds.tacotron_mel_transformer
        dummy_audio = temp_ds[0][1].unsqueeze(0)
        mel_spec = self.mel_transformer.mel_spectrogram(dummy_audio)
        self.output_shape = mel_spec.squeeze(0).T.shape
        self.seq2seq = models.RNNSeq2Seq(self.input_shape, self.output_shape)

    def forward(self, x):
        outputs, _, _ = self.seq2seq(x)
        return outputs

model = Model.load_from_checkpoint(CHECKPOINT_PATH)
model.to('cuda').eval()

# --- 2. PREPARE FULL TEST SET ---
test_ds = dataset.get_data(split='test')
total_samples = len(test_ds)
print(f"‚úÖ Loaded. Processing ALL {total_samples} samples from Test Set.")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# Lists to hold all audio for the final concatenated file
full_pred_audio = []
full_true_audio = []
silence = np.zeros(int(22050 * 0.5)) # 0.5 seconds of silence

# --- 3. GENERATION LOOP ---
for i in tqdm(range(total_samples), desc="Generating Audio"):

    eeg, target_audio = test_ds[i]
    eeg = eeg.unsqueeze(0).to('cuda')

    # Predict & Boost Volume
    with torch.no_grad():
        pred_mel = model(eeg)

        # Transpose for WaveGlow
        pred_mel = pred_mel.transpose(1, 2)

        # Volume Boost (This was the only fix we had at this stage)
        current_max = pred_mel.max()
        target_max = -0.5
        pred_mel = pred_mel + (target_max - current_max)

        # WaveGlow Inference (NO Time Stretch Here)
        audio = waveglow.infer(pred_mel)

    # Convert to Numpy
    audio_pred = audio[0].data.cpu().numpy()
    audio_true = target_audio.cpu().numpy()

    # Save Individual Files
    sf.write(f"{OUTPUT_DIR}/sample_{i}_PRED.wav", audio_pred, 22050)

    # Add to list for concatenation
    full_pred_audio.append(audio_pred)
    full_pred_audio.append(silence)

    full_true_audio.append(audio_true)
    full_true_audio.append(silence)

# --- 4. SAVE CONCATENATED "LONG" FILES ---
print("Concatenating full audio files...")
final_pred = np.concatenate(full_pred_audio)
final_true = np.concatenate(full_true_audio)

sf.write(f"{OUTPUT_DIR}/FULL_TEST_PREDICTION.wav", final_pred, 22050)
sf.write(f"{OUTPUT_DIR}/FULL_TEST_TRUTH.wav", final_true, 22050)

print(f"\nüéâ DONE (Original Version).")